import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import html
import re
import pymysql
import logging
import colorlog
from datetime import datetime
from typing import List, Dict

# æœ¬åœ°æ•°æ®åº“é…ç½®ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
db_config = {
    "host": "localhost",
    "port": 3306,
    "user": "root",
    "password": "root",
    "db": "news_db",
    "charset": "utf8mb4"
}

# é…ç½®å½©è‰²æ—¥å¿—
handler = colorlog.StreamHandler()
handler.setFormatter(colorlog.ColoredFormatter(
    '%(log_color)s%(asctime)s - %(levelname)s - %(message)s',
    log_colors={
        'DEBUG': 'cyan',
        'INFO': 'green',
        'WARNING': 'red',
        'ERROR': 'purple',
        'CRITICAL': 'red,bg_white',
    }
))
logger = colorlog.getLogger()
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
date_pattern = re.compile(r'(\d{4}[-/]?\d{1,2}[-/]?\d{1,2}|\d{1,2}[-/]?\d{1,2}[-/]?\d{4}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥|\d{2}\d{4}/\d{2}|\d{2}\d{4})')
chinese_pattern = re.compile(r'[\u4e00-\u9fa5]')
digit_pattern = re.compile(r'\d')

# æ—¥æœŸæ ¼å¼åˆ—è¡¨
date_formats = ['%Yå¹´%mæœˆ%dæ—¥', '%Y%m%d', '%Y/%m/%d', '%Y-%m-%d', '%Y-%m', '%m%d%Y', '%d%Y/%m']

# æ–°é—»ç±»åˆ«æ˜ å°„
category_mapping = {
    "æ‹›ç”Ÿ": ["æ‹›ç”Ÿ", "å½•å–", "admissions"],
    "ç§‘ç ”": ["å®éªŒ", "å‘ç°", "çªç ´", "discovery"],
    "æ ¡å›­": ["æ ¡å›­", "æ ¡å›­ç”Ÿæ´»", "campus"],
    "å¤§å­¦": ["å¤§å­¦", "å­¦é™¢", "university"],
    "å­¦æœ¯": ["ç ”ç©¶", "å­¦æœ¯", "è®ºæ–‡", "science", "research"],
    "æ´»åŠ¨": ["æ´»åŠ¨", "è®²åº§", "ä¼šè®®", "event", "forum"],
    "å…¬å‘Š": ["é€šçŸ¥", "å…¬å‘Š", "å£°æ˜", "announcement"]
}

def categorize_news(title: str) -> str:
    """åŸºäºæ ‡é¢˜å…³é”®è¯è‡ªåŠ¨åˆ†ç±»"""
    title_lower = title.lower()
    for category, keywords in category_mapping.items():
        if any(kw in title_lower for kw in keywords):
            return category
    return "å…¶ä»–"

def check_url_availability(url: str, timeout: int = 5) -> bool:
    """æ£€æµ‹URLå¯è¾¾æ€§ï¼ˆä½¿ç”¨HEADæ–¹æ³•ï¼‰"""
    try:
        resp = requests.head(url, timeout=timeout, allow_redirects=True)
        return resp.status_code == 200
    except (requests.exceptions.RequestException, ValueError):
        return False

def clean_invalid_links() -> int:
    """æ¸…ç†æ— æ•ˆé“¾æ¥è®°å½•"""
    try:
        with pymysql.connect(**db_config) as connection:
            with connection.cursor() as cursor:
                cursor.execute("SELECT id, link FROM news_info")
                records = cursor.fetchall()
                
                deleted_count = 0
                for record_id, link in records:
                    if not check_url_availability(link):
                        cursor.execute("DELETE FROM news_info WHERE id = %s", (record_id,))
                        deleted_count += 1
                        logger.warning(f"åˆ é™¤æ— æ•ˆé“¾æ¥ï¼š{link[:50]}...")
                
                connection.commit()
                logger.info(f"æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤{deleted_count}æ¡è®°å½•")
                return deleted_count
    except Exception as e:
        logger.error(f"æ¸…ç†å¤±è´¥ï¼š{str(e)}")
        return 0

def parse_date(date_str: str) -> datetime:
    """å¤šç§æ—¥æœŸæ ¼å¼è§£æ"""
    for fmt in date_formats:
        try:
            date_obj = datetime.strptime(date_str, fmt)
            if fmt == '%Y-%m':  # å¤„ç†æ²¡æœ‰æ—¥æœŸçš„æ ¼å¼
                date_obj = date_obj.replace(day=1)
            return date_obj
        except ValueError:
            continue
    raise ValueError("æ— æ³•è¯†åˆ«çš„æ—¥æœŸæ ¼å¼")

def create_news_table() -> bool:
    """åˆ›å»º/æ›´æ–°æ–°é—»ä¿¡æ¯è¡¨ç»“æ„"""
    try:
        with pymysql.connect(**db_config) as connection:
            with connection.cursor() as cursor:
                # åˆ›å»ºæ–°è¡¨ç»“æ„
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS news_info (
                        id INT AUTO_INCREMENT PRIMARY KEY,
                        university_name VARCHAR(255) NOT NULL COMMENT 'å¤§å­¦åç§°',
                        title VARCHAR(191) UNIQUE NOT NULL COMMENT 'æ–°é—»æ ‡é¢˜',
                        date DATE NOT NULL COMMENT 'æ–°é—»æ—¥æœŸ',
                        link VARCHAR(255) NOT NULL COMMENT 'æ–°é—»é“¾æ¥',
                        category VARCHAR(50) COMMENT 'æ–°é—»åˆ†ç±»',
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´'
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
                ''')

                # æ·»åŠ categoryå­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                cursor.execute("SHOW COLUMNS FROM news_info LIKE 'category'")
                if not cursor.fetchone():
                    cursor.execute("ALTER TABLE news_info ADD COLUMN category VARCHAR(50) AFTER link")
                    
                connection.commit()
                logger.info("è¡¨ç»“æ„éªŒè¯/æ›´æ–°æˆåŠŸ")

                # å­—æ®µå®Œæ•´æ€§æ£€æŸ¥
                cursor.execute("DESC news_info")
                field_list = [column[0] for column in cursor.fetchall()]
                required_fields = ['university_name', 'title', 'date', 'link', 'category']
                if all(field in field_list for field in required_fields):
                    logger.info("è¡¨å­—æ®µéªŒè¯é€šè¿‡")
                    return True
                logger.error("è¡¨å­—æ®µç¼ºå¤±")
                return False
    except Exception as e:
        logger.error(f"è¡¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return False

def insert_news_data(data: List[Dict]) -> int:
    """æ’å…¥æœ‰æ•ˆæ•°æ®åˆ°æ•°æ®åº“"""
    inserted_count = 0
    try:
        with pymysql.connect(**db_config) as connection:
            with connection.cursor() as cursor:
                insert_query = """
                    INSERT INTO news_info 
                    (university_name, title, date, link, category)
                    VALUES (%s, %s, %s, %s, %s)
                """

                for item in data:
                    # URLæœ‰æ•ˆæ€§æ£€æŸ¥
                    if not check_url_availability(item['url']):
                        logger.warning(f"é“¾æ¥ä¸å¯è¾¾ï¼š{item['url'][:50]}...")
                        continue

                    # æ„å»ºæ’å…¥æ•°æ®
                    try:
                        cursor.execute(insert_query, (
                            item['university_name'],
                            item['title'],
                            item['date'],
                            item['url'],
                            categorize_news(item['title'])
                        ))
                        inserted_count += 1
                    except pymysql.IntegrityError as e:
                        if e.args[0] == 1062:
                            logger.debug(f"é‡å¤æ ‡é¢˜ï¼š{item['title'][:20]}...")
                        else:
                            raise
                
                connection.commit()
                logger.info(f"æˆåŠŸæ’å…¥{inserted_count}æ¡æ–°è®°å½•")
                return inserted_count
    except Exception as e:
        logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}")
        return 0

def extract_news(html_content: str, base_url: str) -> List[Dict]:
    """ä»HTMLä¸­æå–æ–°é—»æ•°æ®"""
    soup = BeautifulSoup(html_content, 'html.parser')
    news_list = []
    
    for link in soup.find_all('a', href=True):
        # æå–æ ‡é¢˜
        title = html.unescape(link.get('title') or link.get_text(strip=True))
        if not title or len(title) < 8:
            continue

        # è¿‡æ»¤éä¸­æ–‡æ ‡é¢˜
        if not chinese_pattern.search(title):
            continue

        # å¤„ç†é“¾æ¥
        url = urljoin(base_url, link['href'])
        
        # æŸ¥æ‰¾æ—¥æœŸ
        element = link
        date_str = None
        while element and not date_str:
            text = element.get_text()
            match = date_pattern.search(text)
            if match:
                date_str = match.group(1)
            element = element.parent

        # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
        if date_str and url:
            try:
                news_list.append({
                    "title": title,
                    "url": url,
                    "date": parse_date(date_str).strftime('%Y-%m-%d')
                })
            except ValueError:
                logger.warning(f"æ— æ•ˆæ—¥æœŸæ ¼å¼ï¼š{date_str}")
                
    return news_list

sites = [
    ("ä¸Šæµ·å¤§å­¦", "https://news.shu.edu.cn/index/zyxw.htm"),
    ("å¤æ—¦å¤§å­¦", "https://www.fudan.edu.cn/"),
    ("ä¸Šæµ·äº¤é€šå¤§å­¦", "https://news.sjtu.edu.cn/index.html"),
    ("åä¸œå¸ˆèŒƒå¤§å­¦","https://www.ecnu.edu.cn/xwlm/xwrd.htm"),
    ("åŒæµå¤§å­¦", "https://news.tongji.edu.cn/tjyw1.htm"),
    ("ä¸Šæµ·å¸ˆèŒƒå¤§å­¦", "https://xw.shnu.edu.cn/main.htm"),
    ("ä¸Šæµ·å¯¹å¤–ç»è´¸å¤§å­¦", "https://news.suibe.edu.cn/main.htm"),
    ("ä¸Šæµ·äº¤é€šå¤§å­¦", "https://news.sjtu.edu.cn/jdyw/index.html"),
    ("ä¸Šæµ·äº¤é€šå¤§å­¦åŒ»å­¦é™¢é™„å±æ–°ååŒ»é™¢", "https://www.xinhuamed.com.cn/news/index.html"),
    ("ä¸Šæµ·åº”ç”¨æŠ€æœ¯å¤§å­¦", "https://www.sit.edu.cn/xww/")
]

def main():
    if not create_news_table():
        logger.error("âŒ è¡¨åˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return

    logger.info("ğŸš€ å¼€å§‹æŠ“å–æ–°é—»æ•°æ®...")
    for uni_name, url in sites:
        try:
            logger.info(f"æ­£åœ¨å¤„ç†ï¼š{uni_name}")
            resp = requests.get(url, timeout=10)
            resp.encoding = 'utf-8'
            
            if not resp.ok:
                logger.warning(f"è¯·æ±‚å¤±è´¥ï¼š{resp.status_code}")
                continue

            news_items = extract_news(resp.text, url.rsplit('/', 1)[0])
            formatted_data = [{
                "university_name": uni_name,
                "title": n['title'],
                "date": n['date'],
                "url": n['url']
            } for n in news_items]

            inserted = insert_news_data(formatted_data)
            logger.info(f"{uni_name} æ’å…¥{inserted}æ¡æ•°æ®")

        except Exception as e:
            logger.error(f"{uni_name} å¤„ç†å¤±è´¥ï¼š{str(e)[:200]}")

    logger.info("ğŸ§¹ æ‰§è¡Œé“¾æ¥æ¸…ç†...")
    clean_invalid_links()
    logger.info("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ")

if __name__ == "__main__":
    main()